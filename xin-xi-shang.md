#熵
entropy, 熵是随机变量不确定性的度量
$$
H(X)=-\sum_{i=1}^{n}p(x_i)logp(x_i)
$$

#条件熵
在x取值一定的情况下随机变量y不确定性的度量
$$
H(Y|X) = \sum_{x \in X} p(x)H(Y|X=x)
$$

#信息增益
information gain, 信息增益就是熵和特征条件熵的差，就是以某特征A划分数据集D前后的熵(不确定性)的差值
 
    信息增益 =  entroy(前) -  entroy(后)
$$
 g(D,A)= H(D) - H(D|A)
$$


**缺点**：信息增益偏向取值较多的特征
**原因**：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较 偏向取值较多的特征。 缺点：信息增益偏向取值较多的特征
 

#信息增益比
information gain rate

    信息增益比 = 惩罚参数 * 信息增益
$$
 g_R(D,A) = \frac {g(D,A)}{H_A(D)} 
$$
**注意**：其中的$$H_A(D)$$，对于样本集合$$D$$，将当前特征A作为随机变量（取值是特征A的各个特征值），求得的经验熵。
（之前是把集合类别作为随机变量，现在把某个特征作为随机变量，按照此特征的特征取值对集合D进行划分，计算熵HA(D)）
$$
H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$
**缺点**：信息增益比偏向取值较少的特征   
**原因**：当特征取值较少时$$H_A(D)$$的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。
**使用信息增益比**：基于以上缺点，并不是直接选择信息增益率最大的特征，而是先在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。


#基尼指数
基尼指数(基尼不纯度)：表示在样本集合中一个随机选中的样本被分错的概率。
**注意**：基尼指数越小，被分错的概率越小，集合纯度越高。

     基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率
$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}{p_k}^2
$$     

**说明**:

1. pk表示选中的样本属于k类别的概率，则这个样本被分错的概率是(1-pk)
2. 样本集合中有K个类别，一个随机选中的样本可以属于这k个类别中的任意一个，因而对类别就加和
3. 当为二分类是
$$Gini(p) = 2p(1-p) 
  (令a=p,b=1-p; 则a+b=1, Gini(p)=2ab = 1 - (a^2 + b^2))
$$

**样本集合D的Gini指数** ： 假设集合中有K个类别，则：
$$
GINI(D) = 1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})
$$



参考[决策树](https://www.cnblogs.com/muzixi/p/6566803.html)



    